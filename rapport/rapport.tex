\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[french]{babel}

\usepackage{listings}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{placeins}
\usepackage{todonotes}

\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\title{Rapport Traitement des données in-situ \break Équipe Firewall}
\author{Valentin Jonquière, Mathilde Chollon, Calliste Boudoux d'Hautefeuille}

\begin{document}

\maketitle
\pagebreak

\tableofcontents

\pagebreak

\section{Organisation du travail}
Nous travaillons sur le projet grâce à GitHub. Mathilde Chollon a pour pseudo @mchlln, Valentin Jonquière @Vjonquiere et Calliste Boudoux d'Hautefeuille est @Callisteau.
Lors de chaque TD, nous créons des issues, que nous nous répartissons dans le Kanban.
Chaque membre du groupe possède un bout du TD à réaliser et aide les autres s'il finit en avance. Nous avons des branches pour chaque
fonctionnalité et nous utilisons les merge requests afin de vérifier le travail des autres membres. Cela nous permet d'avoir du code
fonctionnel dans la branche principale et de vraiment savoir ce que modifie chaque personne.


\section{Présentation de l'application}

\section{Positionnement selon la taxonomie de Childs et al. (2020)}
\todo[inline]{Rajouter la citation vers l'article de référence}
Childs et al. ont permis de poser les bases du traitement in-situ et proposent dans leur article un plan en six axes permettant de décrire
et caractériser les applications in-situ.


\subsection{Proximité d'intégration (Integration Proximity)}
%Où s'exécute votre traitement par rapport à la simulation ? Est-il intégré directement dans le code (in-line),
%dans un processus séparé sur les mêmes nœuds (in-transit local), ou sur des nœuds dédiés (in-transit
%distant) ? Justifiez votre choix d'architecture
Notre traitement in-situ est directement intégré au le code de simulation. Nous aurions pu déléguer la tâche du traitement 
in-situ à un processus séparé, voire même à des nœuds dédiés, mais nous avons préféré rester sur l'intégration direct, en in-line.
Cette technique semblant plus  simple à mettre en place et à débugger que les autres, nous avons décidé de nous concentrer sur l'in-line.

\subsection{Accès aux ressources (Resource Access)}
%Comment votre traitement in-situ partage-t-il les ressources (CPU, mémoire, réseau) avec la simulation ? Y
%a-t-il compétition pour les ressources ? Comment avez-vous géré ce partage ?
Dans notre code, il n'y a pas de réel partage des ressources entre la simulation et le traitement in-situ. En effet,
 on effectue une partie de la simulation, puis du traitement, puis de la simulation à nouveau et ainsi de suite, jusqu'à avoir traité toute la simulation.

\subsection{Division du travail (Division of Execution)}
%La simulation et le traitement s'exécutent-ils de manière synchrone ou asynchrone ? Le traitement bloque-t-il
%la simulation ? Discutez les implications de votre choix.
Le traitement in-situ s'exécute de manière synchrone avec la simulation. Comme expliqué dans la sous-section précédente, nous alternons les phases de simulation
et de traitement. Les traitements in-situ bloquent donc la simulation. Cela permet d'avoir une implémentation plus simple. De meilleures performances pourraient 
être observées si nous traitions la simulation et le traitement de manière asynchrone, mais nous avons fait le choix de la simplicité
dans un premier temps. Avec plus de temps pour le projet, il aurait été pertinent de comparer les deux techniques.


\subsection{Contrôle des opérations (Operation Controls)}
%Quand et comment déclenchez-vous le traitement ? À intervalles fixes ? Sur des critères conditionnels ? Qui
%contrôle ces déclenchements (la simulation, un script externe) ?

Nous déclenchons le traitement in-situ toutes les k itérations, avec k données par l'utilisateur. Par défaut, cette valeur est à 50, afin d'avoir la même
valeur par défaut que le snapshots. En effet, le but de ce cours étant de comparer ad-hoc et in-situ, pouvoir gérer nos intervalles de snapshots d'in-situ était nécessaire.
Nous passons donc en paramètres de l'application la durée d'intervalle entre les traitements in-situ, puis lors du run, toutes les k itérations de la simulation, le traitement
s'exécute.
\subsection{Type de sortie (Output Type)}
%Quelles sont les sorties de votre workflow ? Des données brutes, des données réduites, des images, des
%statistiques ? Comment ce choix affecte-t-il la flexibilité de l'analyse ultérieure ?
D'après nos analyses ad-hoc, les données présentes dans les snapshots étaient plus pertinentes et nombreuses. Nous avons donc décidé de nous focaliser sur ces données afin de les 
analyser. Nous avons donc produit diverses statistiques, (pression minimale, moyenne, médiane et écart-type), mais aussi un histogramme de la répartition
de la pression. Nous donnons tout de même les pressions enregistrées aux récepteurs des simsos mais nous ne faisons pas de traitement particulier.
Ces données se trouvent dans le format texte, un fichier étant créé toutes les k itérations.
Nous avons également des sauvegardes de slices de la simulation en format ppm. Nous parcourons donc nos données lors du traitement, 
définissons une palette de couleur et écrivons un fichier correspondant aux slices.
Les informations présentes sur les PPM permettent aux développeurs de savoir si leur code est correct, et de faire une première analyse
contrairement aux données textuelles qui seraient plutôt destinées aux métiers, car plus précises.


\subsection{Fréquence de sortie (Output Frequency)}
%À quelle fréquence produisez-vous des sorties ? Comment ce paramètre impacte-t-il le compromis entre
%fidélité temporelle et coût I/O ?
\section{Workflow ad-hoc}

\subsection{Snaphots}
\subsubsection{Snapshots globales}
L'implémentation des snapshots complète repose sur le parcours de du tableau `pnGlobal' afin de sauvegarder la pression à chaque point
donné du maillage. Nous avons choisi de stocker ces informations sous forme de texte avec des fichier de la forme de CSV. Cela facilite
leur lecture que ce soit par nous, développeurs, pour identifier si les valeurs semblent "choérente" lors du développement, mais également
par les autres corps de métiers qui pourraient avoir besoin de les parcourir (et qui n'ont pas forcément les compétences en informatique pour transformer
un binaire en texte). De plus, cela permet de faciliter le changement de système de visualisation. Ici, puisque toutes nos visualisations \textit{ad-hoc} sont faites
en \textit{R}, la lecture d'un fichier type CSV se fait facilement et de mainère directe. Si l'on voulait ajouter des visualisations avec un autre logiciel,
il serait simple de lire les données sur celui-ci puisque les fichiers CSV sont massivement supportés. Cepedant, ce choix d'implémentation aura un impact direct
sur la taille et le temps nécessaire à écrire les snapshots. Comme le but du projet est de produire et comparer avec un workflow in-situ, les snapshots serviront à 
terme uniquement pour la sauvegarde d'état de la simulation. L'impact de ce choix sera donc moindre puisque les visualisations passeront par d'autres systèmes.

C'est pour cela que nous avons ajouté l'option \textit{--save-interval [interval]} permettant de sauvergader les snapshots toutes les \textit{interval} itérations.
Pour la visualisation nous préférerons utiliser les méthodes des parties \ref{part:snapshotsSlices} et \ref{part:slicePPM}.

Il est donc possible de générer des visualisations à partir de ces snapshots avec le fichier \textit{pressure\_map.R}. Celui-ci génère trois heatmaps différentes correspondant
au différents plans du maillage. On retrouve un exemple de visualisation en Figure \ref{}. Sur ce type de visualisation, on retrouve également les statistiques descriptives présentées
en section \ref{}

\todo[inline]{AJOUTER UN EXEMPLE DE FIGURE}
\todo[inline]{AJOUTER REF VERS SECTION STAT DESC}
\subsubsection{Snapshots slice}\label{part:snapshotsSlices}
Afin de limiter l'impact des snapshots, nous avons mis en place un système de \textit{slice}. L'objectif est ici de créer des snapshots spécifiques à la visualisation
afin de diminuer les coûts pour générer nos heatmaps. Pour ce faire nous avons ajouter une fonction \textit{generate\_snapshot\_slice} visible dans le Listing \ref{lst:slices}.
Nous avons ajouté un paramètre \textit{dim} permettant de choisir sur quelle dimension doit être effectuée la coupe. Il est possible de faire des XY, XZ et YZ. La coupe se fait alors sur le milieu du maillage sur le plan
demandé. Nous n'avons pas ajouté de paramètre permettant de choisir l'indice auquel la coupe doit être effectuée mais il serait simple et rapide de l'ajouter. Pour faire fonctionner
nos visualisations avec les slices, nous avons modifié notre fichier \textit{pressure\_map.R} pour qu'il prenne en compte qu'il n'y avait plus que deux dimensions. Les images obtenues
sont similaires à celles générées avec une snapshot complète.



\begin{lstlisting}[caption={Signature de la fonction generate\_snapshot\_slice}, language=c, label={lst:slices}]
int SEMproxy::generate_snapshot_slice(int indexTimeSample,
    int dim, std::ofstream& compression_file)
\end{lstlisting}

\subsection{Sismos}
Au départ, nous ne possédions les données de sismos que d'un seul récepteur. De plus, ces données étaient "perdues",
elles n'étaient pas sauvegardées, il était donc impossible de faire des traitements dessus.
Nous avons donc rajouté des fonctionnalités dans l'application afin de pouvoir gérer plusieurs récepteurs et sauvegarder les données dans un fichier.
Les options \textit{sismos} \textit{set-receivers} et \textit{sismos-folder} nous permettent respectivement d'activer la sauvegarde des sismos,
de choisir un fichier contentant les récepteurs à monitorer et le dossier de sauvegarde du fichier.

Nous avons fixé le format du fichier dès le départ afin de pouvoir se répartir le travail et pouvoir produire des visualisations à partir de données
d'exemple avant d'avoir terminé l'implémentation des sismos.
Lorsque l'on active l'option sismos, nous obtenons donc un fichier contenant sur chaque ligne le temps et la mesure de pression sur chaque récepteur.
C'est un fichier contenant un header

Ce fichier peut ensuite être utilisé à des fins de visualisation, pour produire des courbes représentant la variabilité des pressions aux récepteurs données au cours du temps.
\section{Workflow in-situ}

Ensuite, nous avons implémenté un workflow in-situ : l'analyse et la visualisation des données se fait en direct, il n'y a pas d'écritures de données brutes de l'application 
dans des fichiers. Cela permet de réduire le surcout d'écriture, mais aussi celui de chargement et de visualisation, qui peut être très lourd lorsque l'on charge des dizaines de 
snapshots dépassant le Gigaoctet de stockage.

Nous avons essayé d'avoir le plus d'analyses similaires à celles du workflow ad-hoc afin d'être plus juste sur nos comparaisons en temps.
Nous avons toutefois réduits le nombre d'analyses des sismogrammes, il y a peu de données que l'on enregistre et nous pensons que les analyses que nous produisons à partir des snapshots
seraient plus pertinent pour les utilisateurs métier.

\subsection{Slice PPM}\label{part:slicePPM}

\section{Compression des données}

Dans notre projet, nous avons décidé d'implémenter la compression avec perte, dote "Lossy".
D'autres approches existent, comme la compression sans perte ou "Lossless". 
Il aurait été pertinent de comparer ces deux méthodes afin de voir les avantages et inconvénients de chaque, mais nous n'avons pas pu le faire par manque de temps.
Les données que nous avons compressées sont les snapshots : ce sont les fichiers qui prennent le plus de place, prenant parfois plus d'un Go de stockage.
Les données que nous stockons à l'intérieur sont les coordonnées d'un point ainsi que sa pression. Le nombre de points sauvegardés dépend bien évidement du type de snapshot :
total ou par slice.
Les coordonnées sont des entiers que nous n'allons pas compresser. En revanche nous avons décidé de compresser la pression, en passant d'un \textit{float} (32 bits) à un 
\textit{short int} (16 bits). L'utilisation de plus petits types de données comme les flottants 8 bits ou même 4 bits auraient pu être intéressants pour voir
si la compression faisait vraiment gagner de la place sans trop perdre en précisions. En revanche, les standards utilisés dans l'application rendaient 
l'utilisation de ces types plus compliquée et nous nous sommes contentés de diviser par 2 la taille des nombres stockés.

\section{Comparaison ad-hoc vs in-situ}

\section{Discussions sur la scalabilité}

\section{Conclusion}

\end{document}